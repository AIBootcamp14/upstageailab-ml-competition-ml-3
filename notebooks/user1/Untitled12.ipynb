{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cbda13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: eli5==0.13.0 in c:\\users\\can40\\anaconda3\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\can40\\anaconda3\\lib\\site-packages (from eli5==0.13.0) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\can40\\anaconda3\\lib\\site-packages (from eli5==0.13.0) (1.20.1)\n",
      "Requirement already satisfied: attrs>17.1.0 in c:\\users\\can40\\anaconda3\\lib\\site-packages (from eli5==0.13.0) (20.3.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\can40\\anaconda3\\lib\\site-packages (from eli5==0.13.0) (0.20.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\can40\\anaconda3\\lib\\site-packages (from eli5==0.13.0) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\can40\\anaconda3\\lib\\site-packages (from eli5==0.13.0) (0.24.1)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in c:\\users\\can40\\anaconda3\\lib\\site-packages (from eli5==0.13.0) (3.1.6)\n",
      "Requirement already satisfied: six in c:\\users\\can40\\anaconda3\\lib\\site-packages (from eli5==0.13.0) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\can40\\anaconda3\\lib\\site-packages (from jinja2>=3.0.0->eli5==0.13.0) (2.1.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\can40\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->eli5==0.13.0) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\can40\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->eli5==0.13.0) (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!pip install eli5==0.13.0\n",
    "!apt-get install -y fonts-nanum\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# 데이터 불러오기\n",
    "dt = pd.read_csv('train3.csv', encoding='utf-8')\n",
    "dt_test = pd.read_csv('test3.csv')\n",
    "dt_bus = pd.read_csv('bus_feature.csv')\n",
    "\n",
    "# 반경 300m 내 버스 정류장 개수 계산 함수 정의\n",
    "def count_nearby_bus_stops(x, y, bus_df, radius=300):\n",
    "    distances = np.sqrt((bus_df['X좌표'] - x)**2 + (bus_df['Y좌표'] - y)**2)\n",
    "    return (distances <= radius).sum()\n",
    "\n",
    "# train과 test 데이터에 파생변수로 버스 정류장 개수 추가\n",
    "dt['num_bus_stops_nearby'] = dt.apply(lambda row: count_nearby_bus_stops(row['좌표X'], row['좌표Y'], dt_bus, radius=300), axis=1)\n",
    "dt_test['num_bus_stops_nearby'] = dt_test.apply(lambda row: count_nearby_bus_stops(row['좌표X'], row['좌표Y'], dt_bus, radius=300), axis=1)\n",
    "\n",
    "# 전체 버스 정류장 개수를 train/test에 추가\n",
    "bus_stop_count = dt_bus.shape[0]\n",
    "dt['bus_stop_count'] = bus_stop_count\n",
    "dt_test['bus_stop_count'] = bus_stop_count\n",
    "\n",
    "# train/test 구분용 is_test 컬럼 추가 후 데이터 합치기\n",
    "dt['is_test'] = 0\n",
    "dt_test['is_test'] = 1\n",
    "concat = pd.concat([dt, dt_test], ignore_index=True)\n",
    "\n",
    "# 의미 없는 값들을 결측치로 처리\n",
    "concat['등기신청일자'] = concat['등기신청일자'].replace(' ', np.nan)\n",
    "concat['거래유형'] = concat['거래유형'].replace('-', np.nan)\n",
    "concat['중개사소재지'] = concat['중개사소재지'].replace('-', np.nan)\n",
    "\n",
    "# 결측치가 100만 개 이하인 컬럼만 선택\n",
    "missing = concat.isnull().sum()\n",
    "selected_cols = list(missing[missing <= 1000000].index)\n",
    "concat_select = concat[selected_cols]\n",
    "\n",
    "# '본번', '부번'을 문자열로 변환\n",
    "concat_select['본번'] = concat_select['본번'].astype(str)\n",
    "concat_select['부번'] = concat_select['부번'].astype(str)\n",
    "\n",
    "# 연속형과 범주형 변수 분리\n",
    "continuous_columns = [col for col in concat_select.columns if pd.api.types.is_numeric_dtype(concat_select[col])]\n",
    "categorical_columns = [col for col in concat_select.columns if col not in continuous_columns]\n",
    "\n",
    "# 범주형 변수 결측치는 'NULL'로 채우고, 연속형 변수는 선형 보간으로 결측치 처리\n",
    "concat_select[categorical_columns] = concat_select[categorical_columns].fillna('NULL')\n",
    "concat_select[continuous_columns] = concat_select[continuous_columns].interpolate(method='linear', axis=0)\n",
    "\n",
    "# 이상치 제거 (IQR 방식) - '전용면적' 컬럼 기준\n",
    "def remove_outliers_iqr(df, column):\n",
    "    train_df = df.query('is_test == 0')\n",
    "    test_df = df.query('is_test == 1')\n",
    "\n",
    "    Q1 = train_df[column].quantile(0.25)\n",
    "    Q3 = train_df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    train_df = train_df[(train_df[column] >= lower_bound) & (train_df[column] <= upper_bound)]\n",
    "    return pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "concat_select = remove_outliers_iqr(concat_select, '전용면적')\n",
    "\n",
    "# '시군구'를 '구'와 '동'으로 분리하고 기존 '시군구' 컬럼 제거\n",
    "concat_select['구'] = concat_select['시군구'].map(lambda x: x.split()[1])\n",
    "concat_select['동'] = concat_select['시군구'].map(lambda x: x.split()[2])\n",
    "concat_select.drop(columns=['시군구'], inplace=True)\n",
    "\n",
    "# '계약년월'에서 '계약년'과 '계약월' 분리 후 기존 컬럼 제거\n",
    "concat_select['계약년'] = concat_select['계약년월'].astype(str).str[:4]\n",
    "concat_select['계약월'] = concat_select['계약년월'].astype(str).str[4:]\n",
    "concat_select.drop(columns=['계약년월'], inplace=True)\n",
    "\n",
    "# 강남 여부 파생 변수 생성\n",
    "gangnam = ['강서구', '영등포구', '동작구', '서초구', '강남구', '송파구', '강동구']\n",
    "concat_select['강남여부'] = concat_select['구'].apply(lambda x: 1 if x in gangnam else 0)\n",
    "\n",
    "# 건축년도 기준 신축 여부 변수 생성 (2009년 이후 신축)\n",
    "concat_select['신축여부'] = concat_select['건축년도'].apply(lambda x: 1 if x >= 2009 else 0)\n",
    "\n",
    "# 다시 train과 test 데이터 분리 및 is_test 컬럼 제거\n",
    "dt_train = concat_select.query('is_test == 0').drop(columns=['is_test'])\n",
    "dt_test = concat_select.query('is_test == 1').drop(columns=['is_test'])\n",
    "\n",
    "# test 데이터 타깃 컬럼 임시로 0으로 생성\n",
    "dt_test['target'] = 0\n",
    "\n",
    "# train 데이터의 연속형과 범주형 변수 분리\n",
    "continuous_columns_v2 = [col for col in dt_train.columns if pd.api.types.is_numeric_dtype(dt_train[col])]\n",
    "categorical_columns_v2 = [col for col in dt_train.columns if col not in continuous_columns_v2]\n",
    "\n",
    "print(\"연속형 변수:\", continuous_columns_v2)\n",
    "print(\"범주형 변수:\", categorical_columns_v2)\n",
    "\n",
    "# 범주형 변수 레이블 인코딩 수행\n",
    "label_encoders = {}\n",
    "for col in tqdm(categorical_columns_v2):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(dt_train[col].astype(str))\n",
    "    dt_train[col] = le.transform(dt_train[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "    for label in np.unique(dt_test[col]):\n",
    "        if label not in le.classes_:\n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    dt_test[col] = le.transform(dt_test[col].astype(str))\n",
    "\n",
    "# 타깃 분리 및 학습/검증 데이터 분리 (80:20 비율)\n",
    "y_train = dt_train['target']\n",
    "X_train = dt_train.drop(columns=['target'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2023)\n",
    "\n",
    "# XGBoost 모델 생성 및 학습\n",
    "model = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 검증 데이터에 대해 예측 및 RMSE 평가\n",
    "pred = model.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "print(f'Validation RMSE: {rmse:.2f}')\n",
    "\n",
    "# 중요 변수 시각화\n",
    "importances = pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.barplot(x=importances.values[:20], y=importances.index[:20])\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.show()\n",
    "\n",
    "# 학습된 모델 저장\n",
    "with open('saved_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Permutation Importance 수행\n",
    "perm = PermutationImportance(model, scoring=\"neg_mean_squared_error\", random_state=42, n_iter=3)\n",
    "perm.fit(X_val, y_val)\n",
    "eli5.show_weights(perm, feature_names=X_val.columns.tolist())\n",
    "\n",
    "# 테스트 데이터에서 타깃 제외 후 예측 수행\n",
    "X_test = dt_test.drop(columns=['target'])\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission = pd.DataFrame({\n",
    "    'id': dt_test['id'],\n",
    "    'target': test_pred\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
